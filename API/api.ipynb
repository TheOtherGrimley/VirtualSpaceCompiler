{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API notebook\n",
    "\n",
    "This file is the readable version of the code that will be put into the api and possible seperate files as it grows. Everything below was coded for the purpose of allowing us to send an API a base64 image and use a series of ML algorthms to detect and manipulate objects within the image. Currently for this submission I am focusing on a cup object on the table although the model could be trained with other objects.\n",
    "\n",
    "This file goes along with a h5 file which is the model used that has been trained, as well as a config file. The system config file outline is below in its own box however the best way to understand this is to look at where it is used.\n",
    "\n",
    "The MRCNN library is used to visualise and run detection on the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE NEEDS MODULARISED\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import json\n",
    "import cv2\n",
    "from flask import Flask, request, jsonify\n",
    "from PIL import Image\n",
    "from keras.backend import clear_session\n",
    "import datetime as datetime\n",
    "\n",
    "#----------------------------\n",
    "# RCNN IMPORTS\n",
    "#----------------------------\n",
    "\n",
    "from mrcnn import utils\n",
    "from mrcnn import visualize\n",
    "from mrcnn.visualize import display_images\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn.model import log\n",
    "import cups as cup\n",
    "\n",
    "#----------------------------\n",
    "# RCNN IMPORTS\n",
    "#----------------------------\n",
    "import keypoint.main as kp\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "sys_config=json.load(open(\"config.json\", 'r'))\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config file\n",
    "\n",
    "Values have been removed from this block for privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'api': {'base uri': '/api'},\n",
       " 'model directory': '-',\n",
       " 'weights path': '-/.h5',\n",
       " 'device': '/cpu:0',\n",
       " 'mode': 'inference',\n",
       " 'cup directory': '-'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"api\":{\n",
    "        \"base uri\":\"/api\"\n",
    "    },\n",
    "    \"model directory\":\"-\",\n",
    "    \"weights path\" : \"-/.h5\",\n",
    "    \"device\":\"/cpu:0\",\n",
    "    \"mode\":\"inference\",\n",
    "    \"cup directory\":\"-\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image decoder\n",
    "\n",
    "In order to support as many image types as possible, the API will only accept bas64 encoded images. base64 is an open file type and can therefore be converter easily from and to many other types. The file type as outlined in the documentation will be png for the final PoC however as the api will be open sourced at the end, base64 was the best way to future proof the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import skimage.io\n",
    "\n",
    "# This is requred to change base64 into a numpy ndarray\n",
    "\n",
    "def decode(base64_string):\n",
    "    if isinstance(base64_string, bytes):\n",
    "        base64_string = base64_string.decode(\"utf-8\")\n",
    "    imgdata = base64.b64decode(base64_string)\n",
    "    img = skimage.io.imread(imgdata, plugin='imageio')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Api Config\n",
    "\n",
    "These blocks are used for configurations local to the api that are required. \n",
    "\n",
    "The first is a get_ax() method. This standardises all graphs used and may be removed in future iterations. Inference config allows us to overwrite some values from the model config to suit the system the api is deployed on. create model is used to do just that: create the model we will use. This makes the h5 file into a dataset we can quickly call to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=16):\n",
    "    #This fn essentially allows a base size for graphs below\n",
    "    #Common thing i've seen in notebooks with matplotlib\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = cup.CupConfig() #Both configs are the same, we just use the one in the cup file for ease\n",
    "\n",
    "class InferenceConfig(config.__class__):\n",
    "    # Make sure we only run detection 1 at a time\n",
    "    # This value may be increased when moved to cloud\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    DETECTION_MIN_CONFIDENCE=0.995\n",
    "config = InferenceConfig()\n",
    "\n",
    "def create_cup_model():\n",
    "    with tf.device(sys_config[\"device\"]):\n",
    "        clear_session()\n",
    "        global cup_model\n",
    "        cup_model = modellib.MaskRCNN(mode=sys_config[\"mode\"], model_dir=sys_config[\"cup\"][\"model directory\"],\n",
    "                                  config=config)\n",
    "    try:\n",
    "        print(\"Loading cup weights \", sys_config[\"cup\"][\"weights path\"])\n",
    "        cup_model.load_weights(sys_config[\"cup\"][\"weights path\"], by_name=True)\n",
    "    except:\n",
    "        print(\"Cup weights file unable to be loaded\".format(error))\n",
    "        \n",
    "def create_bowl_model():\n",
    "    with tf.device(sys_config[\"device\"]):\n",
    "        clear_session()\n",
    "        global bowl_model\n",
    "        bowl_model = modellib.MaskRCNN(mode=sys_config[\"mode\"], model_dir=sys_config[\"bowl\"][\"model directory\"],\n",
    "                                  config=config)\n",
    "    try:\n",
    "        print(\"Loading bowl weights \", sys_config[\"bowl\"][\"weights path\"])\n",
    "        bowl_model.load_weights(sys_config[\"bowl\"][\"weights path\"], by_name=True)\n",
    "    except:\n",
    "        print(\"Bowl weights file unable to be loaded\".format(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final return\n",
    "\n",
    "This method is the final return values for the api. This calls to all other methods and compiles their returns into an easily digestible json dump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rois: [N, (y1, x1, y2, x2)] detection bounding boxes\n",
    "def final_RCNN_ret(image, roi, obj):\n",
    "    img=image[roi[0]:roi[2], roi[1]:roi[3]]\n",
    "    if obj == \"cup\":\n",
    "        cropped_imgs.append(img)\n",
    "    centre = find_box_center(roi, image)\n",
    "    return {\"centre\": centre, \"object_type\":obj}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Centre\n",
    "\n",
    "In order to accurately recreate the image in 3D i will need to know the relative and true coordinates of the object in screen space. This method finds the centrepoint of the region of interest and returns a list of all 4 values. We don't need to worry about different x/y sizes of the whole image because it is normalised to 1024x1024 when decoded by the rcnn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_box_center(roi, image):\n",
    "    bl=(roi[1], roi[2])\n",
    "    br=(roi[3], roi[2])\n",
    "    tl=(roi[1], roi[0])\n",
    "    tr=(roi[3], roi[0])\n",
    "    true_y = (roi[0]+roi[2])/2\n",
    "    true_x = (roi[1]+roi[3])/2\n",
    "    print(\"0: \", roi[0],\" | 1: \", roi[1],\" | 2: \", roi[2],\" | 3: \", roi[3],)\n",
    "    print(\"br: \", br, \" | bl: \", bl, \" | tl: \", tl, \" | tr: \", tr, )\n",
    "    rel_roi = []\n",
    "    for i in roi:\n",
    "        #The shape of the image is normalised to square so we don't need to\n",
    "        #worry about different x and y shapes\n",
    "        rel_roi.append((i/image.shape[0])*100)\n",
    "    rel_y = np.round((rel_roi[0]+rel_roi[2])/2, 2)\n",
    "    rel_x = np.round((rel_roi[1]+rel_roi[3])/2, 2)\n",
    "\n",
    "    return ([true_y, true_x, rel_y, rel_x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect and crop boxes\n",
    "\n",
    "So i need to crop out the object to pass to the next model that will give me the rotation and depth. I could have just cropped the detected mask of the object but i found that just removing the refined region of interest would work better as it gives a more consistent view of the full object. This essentially cuts down on mistakes made by the model giving me a more accurate system overall.\n",
    "\n",
    "The method calls the detection from the RCNN model and uses this to generate the RoIs to pass through the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_boxes_and_crop(objects):\n",
    "    global imgs\n",
    "    imgs=[]\n",
    "    for obj in request.json['objects']:\n",
    "        if obj == \"Cup\":\n",
    "            create_cup_model()\n",
    "            detect_cup_in_img()\n",
    "        if obj == \"Bowl\":\n",
    "            create_bowl_model()\n",
    "            detect_bowl_in_img()\n",
    "    #return json.dumps({\"crops\":imgs})\n",
    "    return imgs\n",
    "\n",
    "def detect_cup_in_img():\n",
    "    prediction = cup_model.detect([image])[0]\n",
    "    for roi in prediction['rois']:\n",
    "        plt.axis('off')\n",
    "        imgs.append(final_RCNN_ret(image, roi, \"cup\"))\n",
    "    \n",
    "def detect_bowl_in_img():\n",
    "    prediction = bowl_model.detect([image])[0]\n",
    "    for roi in prediction['rois']:\n",
    "        plt.axis('off')\n",
    "        box_ret = final_RCNN_ret(image, roi, \"bowl\")\n",
    "        imgs.append(box_ret)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _default_hparams():\n",
    "    hparams = tf.contrib.training.HParams(\n",
    "      num_filters=64,  # Number of filters.\n",
    "      num_kp=10,  # Numer of keypoints.\n",
    "\n",
    "      loss_pose=0.2,  # Pose Loss.\n",
    "      loss_con=1.0,  # Multiview consistency Loss.\n",
    "      loss_sep=1.0,  # Seperation Loss.\n",
    "      loss_sill=1.0,  # Sillhouette Loss.\n",
    "      loss_lr=1.0,  # Orientation Loss.\n",
    "      loss_variance=0.5,  # Variance Loss (part of Sillhouette loss).\n",
    "\n",
    "      sep_delta=0.05,  # Seperation threshold.\n",
    "      noise=0.1,  # Noise added during estimating rotation.\n",
    "\n",
    "      learning_rate=1.0e-3,\n",
    "      lr_anneal_start=5000,  # When to anneal in the orientation prediction.\n",
    "      lr_anneal_end=7000,  # When to use the prediction completely.\n",
    "    )\n",
    "    return hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg(array):\n",
    "    count = 0\n",
    "    for i in array:\n",
    "        count+=i\n",
    "    return count / array.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "hp = _default_hparams()\n",
    "\n",
    "def keypoint_predict(input_img, hparams):\n",
    "    #Reset the tensorflow graph each call\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "\n",
    "    #Set the size of the image we need (128x128)\n",
    "    img = tf.placeholder(tf.float32, shape=(1, 128, 128, 4))\n",
    "\n",
    "    with tf.variable_scope(\"KeypointNetwork\"):\n",
    "        #Generate the network based on the placeholder image size\n",
    "        ret = kp.keypoint_network(img, hparams.num_filters, hparams.num_kp, False)\n",
    "\n",
    "    uv = tf.reshape(ret[0], [-1, hparams.num_kp, 2])\n",
    "    z = tf.reshape(ret[1], [-1, hparams.num_kp, 1])\n",
    "    uvz = tf.concat([uv, z], axis=2)\n",
    "    orient =ret[2]\n",
    "\n",
    "    sess = tf.Session()\n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.get_checkpoint_state(sys_config[\"keypoint\"][\"checkpoint\"])\n",
    "\n",
    "    print(\"loading keypoint model...\")\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "    orig = input_img.astype(float) / 255\n",
    "    if orig.shape[2] == 3:\n",
    "        orig = np.concatenate((orig, np.ones_like(orig[:, :, :1])), axis=2)\n",
    "\n",
    "    uv_ret = sess.run(uvz, feed_dict={img: np.expand_dims(orig, 0)})\n",
    "    z_ret = sess.run(z, feed_dict={img: np.expand_dims(orig, 0)})\n",
    "    orient_ret = sess.run(orient, feed_dict={img: np.expand_dims(orig, 0)})\n",
    "\n",
    "    return {\"Average Depth\":str(avg(z_ret[0])[0]), \"Orientation\":orient_ret.tolist(), \"Points\":uv_ret[0].tolist()}\n",
    "\n",
    "def keypoint_on_crops():\n",
    "    kps=[]\n",
    "    print(len(cropped_imgs))\n",
    "    for img in cropped_imgs:\n",
    "        img = cv2.resize(img, dsize=(128, 128), interpolation=cv2.INTER_CUBIC)\n",
    "        kps.append(keypoint_predict(img, hp))\n",
    "    return kps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The main route\n",
    "\n",
    "This is the route that will be hit when someone calls to the API. it calls both the decode and the detect methods and sets everything in motion. \n",
    "\n",
    "This will be changed in the final version of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@app.route(sys_config['api']['base uri'], methods=['PUT'])\n",
    "def detection():\n",
    "    if not request.json:\n",
    "        abort(400)\n",
    "    global cropped_imgs\n",
    "    cropped_imgs = []\n",
    "    global image\n",
    "    image = decode(request.json['image'])\n",
    "    image = cv2.resize(image, dsize=(500, 500), interpolation=cv2.INTER_CUBIC)\n",
    "    rcnn = detect_boxes_and_crop(request.json['objects'])\n",
    "    keypoint = keypoint_on_crops()\n",
    "    return json.dumps({\"crops\":rcnn, \"keypoints\":keypoint}), 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: Do not use the development server in a production environment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [04/Apr/2019 18:44:32] \"\u001b[33mPUT / HTTP/1.1\u001b[0m\" 404 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cup weights  C:\\Users\\AdamG\\OneDrive\\Documents\\Projects\\Uni\\FYP\\API\\logs\\initial_cups\\mask_rcnn_cup_0017.h5\n",
      "0:  110  | 1:  21  | 2:  290  | 3:  290\n",
      "br:  (290, 290)  | bl:  (21, 290)  | tl:  (21, 110)  | tr:  (290, 110)\n",
      "0:  37  | 1:  275  | 2:  223  | 3:  444\n",
      "br:  (444, 223)  | bl:  (275, 223)  | tl:  (275, 37)  | tr:  (444, 37)\n",
      "2\n",
      "loading keypoint model...\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\AdamG\\OneDrive\\Documents\\Projects\\Uni\\FYP\\API\\keypoint\\chkpt2\\model.ckpt-7255\n",
      "loading keypoint model...\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\AdamG\\OneDrive\\Documents\\Projects\\Uni\\FYP\\API\\keypoint\\chkpt2\\model.ckpt-7255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [04/Apr/2019 18:46:17] \"\u001b[37mPUT /api HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cup weights  C:\\Users\\AdamG\\OneDrive\\Documents\\Projects\\Uni\\FYP\\API\\logs\\initial_cups\\mask_rcnn_cup_0017.h5\n",
      "0:  110  | 1:  21  | 2:  290  | 3:  290\n",
      "br:  (290, 290)  | bl:  (21, 290)  | tl:  (21, 110)  | tr:  (290, 110)\n",
      "0:  37  | 1:  275  | 2:  223  | 3:  444\n",
      "br:  (444, 223)  | bl:  (275, 223)  | tl:  (275, 37)  | tr:  (444, 37)\n",
      "2\n",
      "loading keypoint model...\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\AdamG\\OneDrive\\Documents\\Projects\\Uni\\FYP\\API\\keypoint\\chkpt2\\model.ckpt-7255\n",
      "loading keypoint model...\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\AdamG\\OneDrive\\Documents\\Projects\\Uni\\FYP\\API\\keypoint\\chkpt2\\model.ckpt-7255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [04/Apr/2019 18:49:11] \"\u001b[37mPUT /api HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
