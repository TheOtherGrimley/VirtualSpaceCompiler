{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API notebook\n",
    "\n",
    "This file is the readable version of the code that will be put into the api and possible seperate files as it grows. Everything below was coded for the purpose of allowing us to send an API a base64 image and use a series of ML algorthms to detect and manipulate objects within the image. Currently for this submission I am focusing on a cup object on the table although the model could be trained with other objects.\n",
    "\n",
    "This file goes along with a h5 file which is the model used that has been trained, as well as a config file. The system config file outline is below in its own box however the best way to understand this is to look at where it is used.\n",
    "\n",
    "The MRCNN library is used to visualise and run detection on the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE NEEDS MODULARISED\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import json\n",
    "from flask import Flask, request, jsonify\n",
    "from PIL import Image\n",
    "from keras.backend import clear_session\n",
    "import datetime as datetime\n",
    "\n",
    "#----------------------------\n",
    "# RCNN IMPORTS\n",
    "#----------------------------\n",
    "\n",
    "from mrcnn import utils\n",
    "from mrcnn import visualize\n",
    "from mrcnn.visualize import display_images\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn.model import log\n",
    "import cups as cup\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "sys_config=json.load(open(\"config.json\", 'r'))\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config file\n",
    "\n",
    "Values have been removed from this block for privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'api': {'base uri': '/api'},\n",
       " 'model directory': '-',\n",
       " 'weights path': '-/.h5',\n",
       " 'device': '/cpu:0',\n",
       " 'mode': 'inference',\n",
       " 'cup directory': '-'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"api\":{\n",
    "        \"base uri\":\"/api\"\n",
    "    },\n",
    "    \"model directory\":\"-\",\n",
    "    \"weights path\" : \"-/.h5\",\n",
    "    \"device\":\"/cpu:0\",\n",
    "    \"mode\":\"inference\",\n",
    "    \"cup directory\":\"-\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image decoder\n",
    "\n",
    "In order to support as many image types as possible, the API will only accept bas64 encoded images. base64 is an open file type and can therefore be converter easily from and to many other types. The file type as outlined in the documentation will be png for the final PoC however as the api will be open sourced at the end, base64 was the best way to future proof the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import skimage.io\n",
    "\n",
    "# This is requred to change base64 into a numpy ndarray\n",
    "\n",
    "def decode(base64_string):\n",
    "    if isinstance(base64_string, bytes):\n",
    "        base64_string = base64_string.decode(\"utf-8\")\n",
    "    imgdata = base64.b64decode(base64_string)\n",
    "    img = skimage.io.imread(imgdata, plugin='imageio')\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Api Config\n",
    "\n",
    "These blocks are used for configurations local to the api that are required. \n",
    "\n",
    "The first is a get_ax() method. This standardises all graphs used and may be removed in future iterations. Inference config allows us to overwrite some values from the model config to suit the system the api is deployed on. create model is used to do just that: create the model we will use. This makes the h5 file into a dataset we can quickly call to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=16):\n",
    "    #This fn essentially allows a base size for graphs below\n",
    "    #Common thing i've seen in notebooks with matplotlib\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = cup.CupConfig()\n",
    "\n",
    "class InferenceConfig(config.__class__):\n",
    "    # Make sure we only run detection 1 at a time\n",
    "    # This value may be increased when moved to cloud\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    DETECTION_MIN_CONFIDENCE=0.96\n",
    "config = InferenceConfig()\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    with tf.device(sys_config[\"device\"]):\n",
    "        clear_session()\n",
    "        global model\n",
    "        model = modellib.MaskRCNN(mode=sys_config[\"mode\"], model_dir=sys_config[\"model directory\"],\n",
    "                                  config=config)\n",
    "    try:\n",
    "        print(\"Loading weights \", sys_config[\"weights path\"])\n",
    "        model.load_weights(sys_config[\"weights path\"], by_name=True)\n",
    "    except:\n",
    "        print(\"Weights file unable to be loaded\".format(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final return\n",
    "\n",
    "This method is the final return values for the api. This calls to all other methods and compiles their returns into an easily digestible json dump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rois: [N, (y1, x1, y2, x2)] detection bounding boxes\n",
    "def final_ret(image, roi):\n",
    "    img=image[roi[0]:roi[2], roi[1]:roi[3]]\n",
    "    centre = find_box_center(roi, image)\n",
    "    plt.imshow(img)\n",
    "    filename=\"detect_{:%Y%m%dT%H%M%S}.png\".format(datetime.datetime.now())\n",
    "    #plt.savefig(filename)\n",
    "    return json.dumps({\"centre\": centre, \"filename\":filename})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Centre\n",
    "\n",
    "In order to accurately recreate the image in 3D i will need to know the relative and true coordinates of the object in screen space. This method finds the centrepoint of the region of interest and returns a list of all 4 values. We don't need to worry about different x/y sizes of the whole image because it is normalised to 1024x1024 when decoded by the rcnn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_box_center(roi, image):\n",
    "    true_y = (roi[0]+roi[2])/2\n",
    "    true_x = (roi[1]+roi[3])/2\n",
    "    rel_roi = []\n",
    "    for i in roi:\n",
    "        #The shape of the image is normalised to square so we don't need to\n",
    "        #worry about different x and y shapes\n",
    "        rel_roi.append((i/image.shape[0])*100)\n",
    "    rel_y = np.round((rel_roi[0]+rel_roi[2])/2, 2)\n",
    "    rel_x = np.round((rel_roi[1]+rel_roi[3])/2, 2)\n",
    "\n",
    "    return ([true_y, true_x, rel_y, rel_x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect and crop boxes\n",
    "\n",
    "So i need to crop out the object to pass to the next model that will give me the rotation and depth. I could have just cropped the detected mask of the object but i found that just removing the refined region of interest would work better as it gives a more consistent view of the full object. This essentially cuts down on mistakes made by the model giving me a more accurate system overall.\n",
    "\n",
    "The method calls the detection from the RCNN model and uses this to generate the RoIs to pass through the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_boxes_and_crop(image):\n",
    "    create_model()\n",
    "    prediction = model.detect([image])[0]\n",
    "    imgs=[]\n",
    "    for roi in prediction['rois']:\n",
    "        plt.axis('off')\n",
    "        box_ret = final_ret(image, roi)\n",
    "        imgs.append(box_ret)\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The main route\n",
    "\n",
    "This is the route that will be hit when someone calls to the API. it calls both the decode and the detect methods and sets everything in motion. \n",
    "\n",
    "This will be changed in the final version of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@app.route(sys_config['api']['base uri'], methods=['PUT'])\n",
    "def detection_split():\n",
    "    if not request.json:\n",
    "        abort(400)\n",
    "    for obj in request.json['objects']:\n",
    "        if obj == \"Cup\":\n",
    "            detect_cup_in_img()\n",
    "            \n",
    "def detect_cup_in_img():\n",
    "    image = decode(request.json['image'])\n",
    "    cropped_images=detect_boxes_and_crop(image)\n",
    "    return jsonify(cropped_images), 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: Do not use the development server in a production environment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights  C:\\Users\\AdamG\\OneDrive\\Documents\\Projects\\Uni\\FYP\\API\\logs\\initial_cups\\mask_rcnn_cup_0017.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [06/Mar/2019 15:19:01] \"\u001b[37mPUT /api HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights  C:\\Users\\AdamG\\OneDrive\\Documents\\Projects\\Uni\\FYP\\API\\logs\\initial_cups\\mask_rcnn_cup_0017.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [06/Mar/2019 15:52:26] \"\u001b[37mPUT /api HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights  C:\\Users\\AdamG\\OneDrive\\Documents\\Projects\\Uni\\FYP\\API\\logs\\initial_cups\\mask_rcnn_cup_0017.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [06/Mar/2019 15:55:58] \"\u001b[37mPUT /api HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights  C:\\Users\\AdamG\\OneDrive\\Documents\\Projects\\Uni\\FYP\\API\\logs\\initial_cups\\mask_rcnn_cup_0017.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [06/Mar/2019 16:16:35] \"\u001b[37mPUT /api HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights  C:\\Users\\AdamG\\OneDrive\\Documents\\Projects\\Uni\\FYP\\API\\logs\\initial_cups\\mask_rcnn_cup_0017.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [06/Mar/2019 16:31:55] \"\u001b[37mPUT /api HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights  C:\\Users\\AdamG\\OneDrive\\Documents\\Projects\\Uni\\FYP\\API\\logs\\initial_cups\\mask_rcnn_cup_0017.h5\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(debug=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
